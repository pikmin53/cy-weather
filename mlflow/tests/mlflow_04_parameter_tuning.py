"""
Script 4: Monitoring et comparaison de modÃ¨les (Hyperparameter Tuning)
========================================================================
Ce script dÃ©montre comment:
- EntraÃ®ner plusieurs modÃ¨les avec diffÃ©rents hyperparamÃ¨tres
- Logger et comparer les performances
- Utiliser des runs parents/enfants pour organiser les expÃ©rimentations
- Trouver le meilleur modÃ¨le automatiquement
"""

import mlflow
import mlflow.sklearn
from mlflow.tracking import MlflowClient
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, f1_score
import numpy as np
from itertools import product

# Configuration MLflow
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("04-Hyperparameter-Tuning")

# Chargement des donnÃ©es
print("ğŸ“Š Chargement du dataset Iris...")
data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    data.data, data.target, test_size=0.2, random_state=42
)

# DÃ©finir la grille d'hyperparamÃ¨tres Ã  tester
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10]
}

print(f"\nğŸ”¬ Test de {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split'])} combinaisons d'hyperparamÃ¨tres...\n")

# CrÃ©er un run parent pour grouper tous les tests
with mlflow.start_run(run_name="hyperparameter_tuning_experiment") as parent_run:
    
    mlflow.set_tag("experiment_type", "grid_search")
    mlflow.set_tag("model_family", "RandomForest")
    
    best_accuracy = 0
    best_params = None
    best_run_id = None
    all_results = []
    
    # Tester toutes les combinaisons
    for n_est, max_d, min_split in product(
        param_grid['n_estimators'],
        param_grid['max_depth'],
        param_grid['min_samples_split']
    ):
        
        # CrÃ©er un run enfant pour chaque combinaison
        with mlflow.start_run(
            run_name=f"RF_n{n_est}_d{max_d}_s{min_split}",
            nested=True
        ) as child_run:
            
            # ParamÃ¨tres
            params = {
                'n_estimators': n_est,
                'max_depth': max_d,
                'min_samples_split': min_split,
                'random_state': 42
            }
            
            mlflow.log_params(params)
            
            # EntraÃ®nement
            model = RandomForestClassifier(**params)
            model.fit(X_train, y_train)
            
            # PrÃ©dictions
            predictions = model.predict(X_test)
            
            # MÃ©triques
            accuracy = accuracy_score(y_test, predictions)
            f1 = f1_score(y_test, predictions, average='macro')
            
            # Cross-validation score
            cv_scores = cross_val_score(model, X_train, y_train, cv=5)
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            metrics = {
                'accuracy': accuracy,
                'f1_score': f1,
                'cv_mean': cv_mean,
                'cv_std': cv_std
            }
            
            mlflow.log_metrics(metrics)
            
            # Logger le modÃ¨le
            mlflow.sklearn.log_model(model, "model")
            
            # Garder trace du meilleur modÃ¨le
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_params = params
                best_run_id = child_run.info.run_id
            
            all_results.append({
                'params': params,
                'accuracy': accuracy,
                'f1_score': f1,
                'cv_mean': cv_mean,
                'run_id': child_run.info.run_id
            })
            
            print(f"âœ“ n_est={n_est:3d}, max_depth={str(max_d):4s}, min_split={min_split:2d} â†’ Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
    
    # Logger les rÃ©sultats du meilleur modÃ¨le dans le run parent
    mlflow.log_params({f"best_{k}": v for k, v in best_params.items()})
    mlflow.log_metric("best_accuracy", best_accuracy)
    mlflow.set_tag("best_run_id", best_run_id)
    
    print("\n" + "="*70)
    print("ğŸ† MEILLEUR MODÃˆLE TROUVÃ‰")
    print("="*70)
    print(f"Accuracy: {best_accuracy:.4f}")
    print(f"ParamÃ¨tres:")
    for param, value in best_params.items():
        print(f"  - {param}: {value}")
    print(f"Run ID: {best_run_id}")

# ====================
# ANALYSE DES RÃ‰SULTATS
# ====================
print("\n" + "="*70)
print("ğŸ“Š ANALYSE DES RÃ‰SULTATS")
print("="*70)

# Trier par accuracy
all_results_sorted = sorted(all_results, key=lambda x: x['accuracy'], reverse=True)

print("\nğŸ¥‡ TOP 5 DES MODÃˆLES:")
for i, result in enumerate(all_results_sorted[:5], 1):
    print(f"\n{i}. Accuracy: {result['accuracy']:.4f} | F1: {result['f1_score']:.4f} | CV: {result['cv_mean']:.4f}")
    print(f"   ParamÃ¨tres: {result['params']}")

# Statistiques globales
accuracies = [r['accuracy'] for r in all_results]
print(f"\nğŸ“ˆ STATISTIQUES GLOBALES:")
print(f"Accuracy moyenne: {np.mean(accuracies):.4f}")
print(f"Accuracy mÃ©diane: {np.median(accuracies):.4f}")
print(f"Accuracy min: {np.min(accuracies):.4f}")
print(f"Accuracy max: {np.max(accuracies):.4f}")
print(f"Ã‰cart-type: {np.std(accuracies):.4f}")

# ====================
# REQUÃŠTE MLFLOW POUR RETROUVER LES MEILLEURS RUNS
# ====================
print("\n" + "="*70)
print("ğŸ” RECHERCHE AVEC L'API MLFLOW")
print("="*70)

client = MlflowClient()

# Rechercher les runs avec une accuracy > 0.95
high_accuracy_runs = client.search_runs(
    experiment_ids=[mlflow.get_experiment_by_name("04-Hyperparameter-Tuning").experiment_id],
    filter_string="metrics.accuracy > 0.95",
    order_by=["metrics.accuracy DESC"],
    max_results=5
)

print(f"\nğŸ¯ Runs avec Accuracy > 0.95 ({len(high_accuracy_runs)} trouvÃ©s):")
for run in high_accuracy_runs:
    if run.data.metrics:  # VÃ©rifier que le run a des mÃ©triques
        accuracy = run.data.metrics.get('accuracy', 'N/A')
        n_est = run.data.params.get('n_estimators', 'N/A')
        max_d = run.data.params.get('max_depth', 'N/A')
        print(f"  - Accuracy: {accuracy:.4f} | n_estimators={n_est}, max_depth={max_d}")

print("\nğŸ‰ ExpÃ©rience terminÃ©e!")
print("ğŸ’¡ Consultez MLflow UI pour visualiser les comparaisons graphiques.")
print("ğŸ’¡ Dans l'UI, utilisez 'Compare' pour voir les runs cÃ´te Ã  cÃ´te.")
